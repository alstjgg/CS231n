{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Introduction to Neural Networks\n",
    "Given a function $f(x)$ where $x$ is a vector of inputs, what is the gradient of $f$ at $x$?\n",
    "\n",
    "In otherwords, compute $\\nabla_x f$\n",
    "\n",
    "In neural networs, $f$ is the loss function $L$, while $x$ is the training data(image) and weights.\n",
    "***\n",
    "\n",
    "# Computational graph\n",
    "\n",
    "![image.png](https://raw.githubusercontent.com/alstjgg/alstjgg.github.io/master/CS231n/04.computational_graph.PNG)\n",
    "\n",
    "A **computational graph** can represent any mathematical function visually.\n",
    "\n",
    "Each **node** represents the steps of computation, such as multiplication or addition. Here, $x$, $y$, and $z$ are **inputs** of the function, while $f$ is the final result value. $q$ is an intermediate node.\n",
    "\n",
    "### Advantage\n",
    "- **Backpropagation** is possible, allowing fast and easy computation of the gradient\n",
    "- Useful for complex mathematical functions used in models such as convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation\n",
    "**Forward Propagation** computes the result of the function and any intermediate values an \n",
    "\n",
    "**Back Propagation** is a method of computing gradients of expressions through recursive application of the **chain rule**.\n",
    "\n",
    "The objective is to compute the gradient of $f$ respect to each input variable\n",
    "\n",
    "### For a Single node\n",
    "All nodes of a computational graph are only aware of their immediate surroundings; the **local input values** and **local output value**. Therefore, the **local gradients** and **upstream gradient** can be easily computed.\n",
    "\n",
    "![image.png](https://raw.githubusercontent.com/alstjgg/alstjgg.github.io/master/CS231n/04.back_propagation.PNG)\n",
    "\n",
    "- input values : $x$, $y$\n",
    "- function : $f$\n",
    "- output value : $z$\n",
    "- local gradients : $\\frac{\\delta z}{\\delta x}$, $\\frac{\\delta z}{\\delta y}$\n",
    "- upstream gradienst: $\\frac{\\delta L}{\\delta z}$\n",
    "    - These have been computed in the previous backpropagation process\n",
    "- objective: compute the gradient of $L$ respect to every input variable\n",
    "\n",
    "Using the local gradients, we can compute the gradient of $f$ respect to each intermediate value. The chain rule can come into use.\n",
    "\n",
    "Assigning actual values for each variables(node) of the graph proves that using the computational graph and backpropagation is much easier in computing the gradient of $f$.\n",
    "\n",
    "#### Some observations\n",
    "- Although nodes can be some of the simplest mathematical functions(addition, multiplication, etc), **functions can be grouped together** to form more complex ones.\n",
    "\n",
    "    ![image.png](https://raw.githubusercontent.com/alstjgg/alstjgg.github.io/master/CS231n/04.sigmoid_function.PNG)\n",
    "\n",
    "    For example, we can create a sigmoid gate by groupding.\n",
    "    \n",
    "- **add** gate : gradient distributer (the upstream gradient is equally passed along each branch)\n",
    "- **max** gate : gradient router (the upstream gradient is passed to one branch, while the other branch gets passed the value 0)\n",
    "- **mul** gate : gradient switcher/scaler (the upstream gradient is scaled by the value of the branch)\n",
    "- Gradients can get added at branches, so changes in one node can effect the whole graph.\n",
    "- Each gradient represents how sensitive the function is to that certain element\n",
    "\n",
    "### For Vectorized codes\n",
    "- Variables are vectors in **vectorized codes**\n",
    "- The gradients becomes the **jacobian matrix**, a matrix where each element is the derivative of the values of the vector\n",
    "\n",
    "#### Some observations\n",
    "- The size of the jacobian matrix is same as the input matrix size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularized implementation\n",
    "The computation process can be seen as a forward and backward API.\n",
    "\n",
    "The **forward** API computes the values of each node, while the **backward** API computes the gradients of each node.\n",
    "\n",
    "```python\n",
    "class ComputationalGraph(object):\n",
    "    def forward(inputs):\n",
    "        for node in self.graph.nodes_topologically_sorted():\n",
    "            node.forward()\n",
    "        return loss    # the final node outputs the loss\n",
    "    \n",
    "    def backward():\n",
    "        for node in reversed(self.graph.nodes_topologically_sorted()):\n",
    "            node.backward()\n",
    "        return inputs_gradients\n",
    "```\n",
    "\n",
    "### Computation node implementation\n",
    "\n",
    "```python\n",
    "class MultiplyNode(object):\n",
    "    def forward(x, y):\n",
    "        z = x*y\n",
    "        # cache values to use in future\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return z\n",
    "    def backward(dz):    # upstream gradient dz = dL / dz\n",
    "        dx = self.y * dz    # local gradient\n",
    "        dy = self.x * dz    # local gradient\n",
    "        return [dx, dy]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
